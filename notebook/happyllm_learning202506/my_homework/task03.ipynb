{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b8eb24",
   "metadata": {},
   "source": [
    "# Task03 预训练语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef98923",
   "metadata": {},
   "source": [
    "## 1 Encoder-only PLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c0ca5a",
   "metadata": {},
   "source": [
    "### 1.1 BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93cf88c",
   "metadata": {},
   "source": [
    "- 核心思想：基于Transformer架构，通过将Encoder结构进行堆叠，扩大模型参数；基于双向LSTM架构，在训练数据上预训练，针对下游任务进行微调。\n",
    "- 模型架构：\n",
    "    1. 在模型的最顶层加入了一个分类头 prediction_heads（线性层+激活函数），用于将多维度的隐藏状态通过线性层转换到分类维度。\n",
    "    2. 激活函数是GELU函数：$\\displaystyle \\text{GELU}(x) = 0.5x(1 + tanh(\\sqrt{\\frac{2}{\\pi}})(x + 0.044715x^3))$\n",
    "    3. 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数。\n",
    "    4. 在完成注意力分数的计算之后，先通过 Position Embedding 层来融入相对位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5e3e8",
   "metadata": {},
   "source": [
    "- MLM + NSP（掩码语言模型 + 下一句预测）：\n",
    "    1. 在MLM训练时，随机选择训练语料中15%的token用于遮蔽，有80%概率被遮蔽，10%被替换为任意一个token，10%保持不变。\n",
    "    2. NSP主要用于训练模型在句级的语义关系拟合。\n",
    "    3. 3.3B token的训练语料。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8f47e",
   "metadata": {},
   "source": [
    "- 下游任务微调：更通用的输入和输出层来适配多任务下的迁移学习。\n",
    "    1. 文本分类任务：直接修改模型结构中的 prediction_heads 最后的分类头。\n",
    "    2. 序列标注任务：集成 BERT 多层的隐含层向量再输出最后的标注结果。\n",
    "    3. 文本生成任务：将Encoder的输出直接解码得到最终生成结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83f379",
   "metadata": {},
   "source": [
    "### 1.2 RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b4c1a",
   "metadata": {},
   "source": [
    "- 模型架构：使用 BERT-large（24层 Encoder Layer、1024的隐藏层维度，总参数量 340M）的模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbff003",
   "metadata": {},
   "source": [
    "- 优化方案：\n",
    "    1. 去掉 NSP 预训练任务：将 Mask 操作放在训练阶段中，即动态遮蔽策略。\n",
    "    2. 使用更大规模的预训练数据和预训练步长：使用160GB的预训练数据，训练步长达到500K Step。\n",
    "    3. 更大的 BEP （字节对编码）词表：使用大小约为 50K 的词表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c89a2",
   "metadata": {},
   "source": [
    "### 1.3 ALBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0684eff0",
   "metadata": {},
   "source": [
    "- 优化方案：\n",
    "    1. 将 Embedding 参数进行分解：在 Embedding 层的后面加入一个线性矩阵进行维度变换。\n",
    "    2. 跨层进行参数共享：仅初始化了一个 Encoder 层。\n",
    "    3. 提出 SOP 预训练任务：正例同样由两个连续句子组成，但负例是将这两个的顺序反过来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2bde3",
   "metadata": {},
   "source": [
    "## 2 Encoder-Decoder PLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca217484",
   "metadata": {},
   "source": [
    "### 2.1 T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37496af7",
   "metadata": {},
   "source": [
    "- 核心思想：使用基于Tranformer的Encoder和Decoder，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。\n",
    "- 模型架构：\n",
    "    1. 编码器用于处理输入文本，解码器用于生成输出文本。\n",
    "    2. LayerNorm 采用了 RMSNorm：$\\displaystyle \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}w_i^2 + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebfdd93",
   "metadata": {},
   "source": [
    "- 预训练任务：基于MLM的预训练任务（随机遮蔽15%的token），使用Colossal Clean Crawled Corpus（C4）大规模数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09f727",
   "metadata": {},
   "source": [
    "- 大一统思想：所有的 NLP 任务都可以统一为文本到文本的任务。对于不同的NLP任务，每次输入前都会加上一个任务描述前缀，明确指定当前任务的类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66468f1f",
   "metadata": {},
   "source": [
    "## 3 Decoder-Only PLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f0f2a",
   "metadata": {},
   "source": [
    "### 3.1 GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf02864",
   "metadata": {},
   "source": [
    "- 模型架构：\n",
    "    1. GPT 使用Decoder-Only 结构，适用于文本生成任务。\n",
    "    2. GPT 沿用了 Transformer 的经典 Sinusoidal 位置编码。\n",
    "    3. 掩码自注意力的计算：在计算得到注意力权重之后，通过掩码矩阵来遮蔽了未来 token 的注意力权重，从而限制每一个 token 只能关注到它之前 token 的注意力。\n",
    "    4. GPT 的 MLP 层没有选择线性矩阵来进行特征提取，而是选择了两个一维卷积核来提取。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb4393",
   "metadata": {},
   "source": [
    "- 预训练任务：使用 CLM（因果语言模型），基于一个自然语言序列的前面所有 token 来预测下一个 token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783fea5a",
   "metadata": {},
   "source": [
    "### 3.2 LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c0a3b",
   "metadata": {},
   "source": [
    "- 模型架构：\n",
    "    1. Attention：模型会分别计算query、key和value这三个向量，与GPT不同的就在这个Query和Key的计算。\n",
    "    2. MLP：通过两个全连接层对hidden_states进行进一步的特征提取。第一个全连接层将hidden_states映射到一个中间维度，然后通过激活函数进行非线性变换。第二个全连接层则将特征再次映射回原始的hidden_states维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0a6ef",
   "metadata": {},
   "source": [
    "### 3.3 GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbb670",
   "metadata": {},
   "source": [
    "- 模型架构：\n",
    "    1. 使用 Post Norm 而非 Pre Norm。\n",
    "    2. 使用单个线性层实现最终 token 的预测，而不是使用 MLP。\n",
    "    3. 激活函数从 ReLU 换成了 GeLUS。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84837c32",
   "metadata": {},
   "source": [
    "- 预训练任务：\n",
    "    1. 结合自编码思想和自回归思想的预训练方法。\n",
    "    2. 优化自回归空白填充任务来实现 MLM 与 CLM 思想的结合，每次遮蔽一连串 token。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
