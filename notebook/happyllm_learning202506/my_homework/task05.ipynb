{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d1073d",
   "metadata": {},
   "source": [
    "# Task05 动手搭建大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164e257",
   "metadata": {},
   "source": [
    "## 1 实现LLaMA2大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a58ed",
   "metadata": {},
   "source": [
    "### 1.1 定义超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b652289",
   "metadata": {},
   "source": [
    "主要是为了导出该模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ea9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class ModelConfig(PretrainedConfig):\n",
    "    model_type = \"Tiny-K\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int = 768, # 模型维度\n",
    "            n_layers: int = 12, # Transformer的层数\n",
    "            n_heads: int = 16, # 注意力机制的头数\n",
    "            n_kv_heads: int = 8, # 键值头的数量\n",
    "            vocab_size: int = 6144, # 词汇表大小\n",
    "            hidden_dim: int = None, # 隐藏层维度\n",
    "            multiple_of: int = 64, \n",
    "            norm_eps: float = 1e-5, # 归一化层的eps\n",
    "            max_seq_len: int = 512, # 最大序列长度\n",
    "            dropout: float = 0.0, # dropout概率\n",
    "            flash_attn: bool = True, # 是否使用Flash Attention\n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norm_eps = norm_eps\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "        self.flash_attn = flash_attn\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc1e6a",
   "metadata": {},
   "source": [
    "### 1.2 构建 RMSNorm 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464f44e",
   "metadata": {},
   "source": [
    "RMSNorm公式如下：\n",
    "$$\n",
    "\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\displaystyle \\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} \\cdot \\gamma\n",
    "$$\n",
    "其中，$\\gamma$表示self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787bf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    RMSNorm公式：\n",
    "    y = x * w / sqrt(mean(x^2) + eps)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        # eps是为了防止除以0的情况\n",
    "        self.eps = eps\n",
    "        # weight是一个可学习的参数，全部初始化为1\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # 计算RMSNorm的核心部分\n",
    "        # x.pow(2).mean(-1, keepdim=True)计算了输入x的平方的均值\n",
    "        # torch.rsqrt是平方根的倒数，这样就得到了RMSNorm的分母部分，再加上eps防止分母为0\n",
    "        # 最后乘以x，得到RMSNorm的结果\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward函数是模型的前向传播\n",
    "        # 首先将输入x转为float类型，然后进行RMSNorm，最后再转回原来的数据类型\n",
    "        # 最后乘以weight，这是RMSNorm的一个可学习的缩放因子\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc962df4",
   "metadata": {},
   "source": [
    "我们用一个示例试试看，是否得到一样的维度？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0528a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "args = ModelConfig()\n",
    "\n",
    "norm = RMSNorm(args.dim, args.norm_eps)\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "output = norm(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492dd19",
   "metadata": {},
   "source": [
    "### 1.3 构建 LLaMA2 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed3559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将键和值的维度扩展到和查询的维度一样\n",
    "    :param x: 输入张量，形状为(bs, slen, n_kv_heads, head_dim)\n",
    "    :param n_rep: 重复次数\n",
    "    \"\"\"\n",
    "    # 获取输入张量的形状：批量大小、序列长度、键/值对头的数量、每个头的维度大小\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "\n",
    "    # 如果重复次数为1，则不需要重复，直接返回原始张量\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "\n",
    "    # 对张量进行扩展和重塑操作以重复键值对\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # 在第四个维度（头的维度前）添加一个新的维度\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # 将新添加的维度扩展到n_rep大小，实现重复的效果\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)  # 重新塑形，合并键/值对头的数量和重复次数的维度\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f25d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    计算旋转嵌入的频率\n",
    "    :param dim: 维度，该值是dim//n_head，因为我们是对每个head进行旋转嵌入\n",
    "    :param end: 结束位置\n",
    "    :param theta: 频率缩放因子\n",
    "    :return: 频率的余弦值和正弦值\n",
    "    \"\"\"\n",
    "    # 生成了一个从0开始，步长为2的序列，该序列的长度为dim的一半\n",
    "    seq = torch.arange(0, dim, 2)[: (dim // 2)].float()\n",
    "    # 然后每个元素除以dim，再取theta的倒数，得到为了生成适合旋转嵌入的频率\n",
    "    freqs = 1.0 / (theta ** (seq / dim))\n",
    "    # 生成一个从0到end的时间序列，长度为end\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    # 计算外积，得到一个二维矩阵，每一行是t的元素乘以freqs的元素\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # 计算频率的余弦值，得到实部\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    # 计算频率的正弦值，得到虚部\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf97c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    重塑freqs_cis的形状，使其与x的形状相匹配\n",
    "    :param freqs_cis: 频率的余弦值和正弦值\n",
    "    :param x: 输入张量\n",
    "    :return: 重塑后的freqs_cis\n",
    "    \"\"\"\n",
    "    # 获取x的维度数\n",
    "    ndim = x.ndim\n",
    "\n",
    "    # 断言，确保1在x的维度范围内\n",
    "    assert 0 <= 1 < ndim\n",
    "\n",
    "    # 断言，确保freqs_cis的形状与x的第二维和最后一维相同\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "\n",
    "    # 构造一个新的形状，除了第二维和最后一维，其他维度都为1，这样做是为了能够将freqs_cis与x进行广播操作\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "\n",
    "    # 将freqs_cis调整为新的形状，并返回\n",
    "    return freqs_cis.view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8d2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def apply_rotary_emb(\n",
    "        xq: torch.Tensor,\n",
    "        xk: torch.Tensor,\n",
    "        freqs_cos: torch.Tensor,\n",
    "        freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    旋转嵌入\n",
    "    :param xq: Query张量\n",
    "    :param xk: Key张量\n",
    "    :param freqs_cos: 频率的余弦值\n",
    "    :param freqs_sin: 频率的正弦值\n",
    "    :return: 旋转嵌入后的Query和Key张量\n",
    "    \"\"\"\n",
    "    # 将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "\n",
    "    # 重新塑形频率张量以进行广播\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    # 应用旋转，分别计算旋转后的实部和虚部\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    # 将最后两个维度合并，并还原为原始张量的形状\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13ff699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 24]) torch.Size([50, 24])\n",
      "Query: torch.Size([1, 50, 6, 48])\n",
      "Key: torch.Size([1, 50, 6, 48])\n"
     ]
    }
   ],
   "source": [
    "xq = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "xk = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "\n",
    "# 使用 precompute_freqs_cis 函数获取 sin和cos\n",
    "cos, sin = precompute_freqs_cis(288//6, 50)\n",
    "print(cos.shape, sin.shape)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, cos, sin)\n",
    "\n",
    "print(\"Query:\", xq_out.shape)\n",
    "print(\"Key:\", xk_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0583e56",
   "metadata": {},
   "source": [
    "将上面的函数都利用起来，组装 LLaMA2 Attention 模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72204376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    注意力机制模块，用于计算查询（Q）、键（K）和值（V）之间的注意力权重，并将其应用于值。\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "        # 根据是否指定n_kv_heads，确定用于键（key）和值（value）的头的数量。\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # 确保总头数可以被键值头数整除。\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "\n",
    "        # 模型并行处理大小，默认为1。\n",
    "        model_parallel_size = 1\n",
    "        # 本地计算头数，等于总头数除以模型并行处理大小。\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # 本地键值头数，等于键值头数除以模型并行处理大小。\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        # 重复次数，用于扩展键和值的尺寸。\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        # 每个头的维度，等于模型维度除以头的总数。\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # 定义权重矩阵。\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        # 输出权重矩阵。\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        # 定义dropout。\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        # 保存dropout概率。\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        # 检查是否使用Flash Attention（需要PyTorch >= 2.0）。\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            # 若不支持Flash Attention，则使用手动实现的注意力机制，并设置mask。\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # 创建一个上三角矩阵，用于遮蔽未来信息。\n",
    "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            # 注册为模型的缓冲区\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        # 获取批次大小和序列长度，[batch_size, seq_len, dim]\n",
    "        bsz, seq_len, _ = x.shape\n",
    "\n",
    "        # 计算查询（Q）、键（K）、值（V）。\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # 调整形状以适应头的维度。\n",
    "        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # 应用旋转位置嵌入（RoPE）。\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "        # 对键和值进行扩展以适应重复次数。\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        # 将头作为批次维度处理。\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # 根据是否支持Flash Attention，选择实现方式。\n",
    "        if self.flash:\n",
    "            # 使用Flash Attention。\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None,\n",
    "                                                                      dropout_p=self.dropout if self.training else 0.0,\n",
    "                                                                      is_causal=True)\n",
    "        else:\n",
    "            # 使用手动实现的注意力机制。\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seq_len, :seq_len]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores, xv)\n",
    "\n",
    "        # 恢复时间维度并合并头。\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
    "\n",
    "        # 最终投影回残差流。\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ec505",
   "metadata": {},
   "source": [
    "验证上面的代码是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7aa1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# 创建Attention实例\n",
    "attention_model = Attention(args)\n",
    "\n",
    "# 模拟输入数据\n",
    "batch_size = 1\n",
    "seq_len = 50  # 假设实际使用的序列长度为50\n",
    "dim = args.dim\n",
    "x = torch.rand(batch_size, seq_len, dim)  # 随机生成输入张量\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "# 运行Attention模型\n",
    "output = attention_model(x, freqs_cos, freqs_sin)\n",
    "\n",
    "# attention出来之后的形状 依然是[batch_size, seq_len, dim]\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63889d82",
   "metadata": {},
   "source": [
    "### 1.4 构建 LLaMA2 MLP模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8280a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    多层感知机（MLP）模块。\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # 如果没有指定隐藏层的维度，我们将其设置为输入维度的4倍\n",
    "        # 然后将其减少到2/3，最后确保它是multiple_of的倍数\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dim\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        # 定义第一层线性变换，从输入维度到隐藏维度\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        # 定义第二层线性变换，从隐藏维度到输入维度\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        # 定义第三层线性变换，从输入维度到隐藏维度\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        # 定义dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数\n",
    "        # 首先，输入x通过第一层线性变换和SILU激活函数\n",
    "        # 然后，结果乘以输入x通过第三层线性变换的结果\n",
    "        # 最后，通过第二层线性变换和dropout层\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0e08d",
   "metadata": {},
   "source": [
    "验证上面的代码是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e3be61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "args = ModelConfig()\n",
    "\n",
    "# 创建MLP实例\n",
    "mlp = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)\n",
    "# 随机生成数据\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "# 运行MLP模型\n",
    "output = mlp(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e6d529",
   "metadata": {},
   "source": [
    "### 1.5 构建LLaMA2 Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f51568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    解码器层，包含多头注意力和前馈神经网络\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_id: int, args: ModelConfig):\n",
    "        super().__init__()\n",
    "        # 定义多头注意力的头数\n",
    "        self.n_heads = args.n_heads\n",
    "        # 定义输入维度\n",
    "        self.dim = args.dim\n",
    "        # 定义每个头的维度，等于输入维度除以头数\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        # 定义LLaMA2Attention对象，用于进行多头注意力计算\n",
    "        self.attention = Attention(args)\n",
    "        # 定义LLaMA MLP对象，用于进行前馈神经网络计算\n",
    "        self.feed_forward = MLP(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            dropout=args.dropout,\n",
    "        )\n",
    "        # 定义层的ID\n",
    "        self.layer_id = layer_id\n",
    "        # 定义注意力计算的归一化层\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # 定义前馈神经网络计算的归一化层\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin):\n",
    "        # 前向传播函数\n",
    "        # 首先，输入x经过注意力归一化层，然后进行注意力计算，结果与输入x相加得到h\n",
    "        # 然后，h经过前馈神经网络归一化层，然后进行前馈神经网络计算，结果与h相加得到输出\n",
    "        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fd2a3",
   "metadata": {},
   "source": [
    "验证上面的代码是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3396c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "args = ModelConfig()\n",
    "\n",
    "# 创建LLaMADecoderLayer实例\n",
    "decoderlayer = DecoderLayer(0, args)\n",
    "\n",
    "# 模拟输入数据\n",
    "dim = args.dim\n",
    "seq_len = 50\n",
    "\n",
    "x = torch.randn(1, seq_len, dim) # [bs, seq_len, dim]\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "out = decoderlayer(x, freqs_cos, freqs_sin)\n",
    "\n",
    "# 形状和输入的x一样 [batch_size, seq_len, dim]\n",
    "print(out.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dedc9aa",
   "metadata": {},
   "source": [
    "### 1.6 构建 LLaMA2 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a21374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "857d7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA2(PreTrainedModel):\n",
    "    # 配置类\n",
    "    config_class = ModelConfig\n",
    "    # 记录最后一次计算的损失\n",
    "    last_loss: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, args: ModelConfig = None):\n",
    "        super().__init__(args)\n",
    "        # 初始化模型参数\n",
    "        self.args = args\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = args.vocab_size\n",
    "        # 层数\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        # Decoder层\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(DecoderLayer(layer_id, args))\n",
    "        # 归一化层\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # 输出层\n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # 将词嵌入层的权重与输出层的权重共享\n",
    "        self.tok_embeddings.weight = self.output.weight\n",
    "\n",
    "        # 预计算相对位置嵌入的频率\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(self.args.dim // self.args.n_heads, self.args.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "\n",
    "        # 初始化所有权重\n",
    "        self.apply(self._init_weights)\n",
    "        # 对残差投影进行特殊的缩放初始化\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * args.n_layers))\n",
    "\n",
    "        # 初始化最后一次前向传播的损失属性\n",
    "        self.last_loss = None\n",
    "        # 输出容器\n",
    "        self.OUT = CausalLMOutputWithPast()\n",
    "        # 不分割的模块列表\n",
    "        self._no_split_modules = [name for name, _ in self.named_modules()]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # 初始化权重的函数\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None, **keyargs) -> CausalLMOutputWithPast:\n",
    "        \"\"\"\n",
    "        前向传播函数，用于计算模型的输出。\n",
    "        :param tokens: 输入 token 张量。\n",
    "        :param targets: 目标 token 张量。\n",
    "        :param keyargs: 其他关键字参数。\n",
    "        :return: 模型的输出，包含 logits 和损失。\n",
    "        \"\"\"\n",
    "        if 'input_ids' in keyargs:\n",
    "            tokens = keyargs['input_ids']\n",
    "        if 'attention_mask' in keyargs:\n",
    "            targets = keyargs['attention_mask']\n",
    "\n",
    "        # 前向传播函数\n",
    "        _bsz, seq_len = tokens.shape\n",
    "        # 通过词嵌入层和Dropout层\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        h = self.dropout(h)\n",
    "        # 获取相对位置嵌入的频率\n",
    "        freqs_cos = self.freqs_cos[:seq_len]\n",
    "        freqs_sin = self.freqs_sin[:seq_len]\n",
    "\n",
    "        # 通过Decoder层\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        # 通过归一化层\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if targets is not None:\n",
    "            # 如果给定了目标，计算损失\n",
    "            logits = self.output(h)\n",
    "            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0,\n",
    "                                             reduction='none')\n",
    "        else:\n",
    "            # 推理时的小优化：只对最后一个位置的输出进行前向传播\n",
    "            logits = self.output(h[:, [-1], :])\n",
    "            self.last_loss = None\n",
    "\n",
    "        # 设置输出\n",
    "        self.OUT.__setitem__('logits', logits)\n",
    "        self.OUT.__setitem__('last_loss', self.last_loss)\n",
    "        return self.OUT\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, idx, stop_id=None, max_new_tokens=256, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        给定输入序列 idx（形状为 (bz,seq_len) 的长整型张量），通过多次生成新 token 来完成序列。\n",
    "        在 model.eval() 模式下运行。效率较低的采样版本，没有使用键k/v cache。\n",
    "        \"\"\"\n",
    "        index = idx.shape[1]\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列上下文过长，截断它到最大长度\n",
    "            idx_cond = idx if idx.size(1) <= self.args.max_seq_len else idx[:, -self.args.max_seq_len:]\n",
    "\n",
    "            # 前向传播获取序列中最后一个位置的 logits\n",
    "            logits = self(idx_cond).logits\n",
    "            logits = logits[:, -1, :]  # 只保留最后一个时间步的输出\n",
    "\n",
    "            if temperature == 0.0:\n",
    "                # 选择最有可能的索引\n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                # 缩放 logits 并应用 softmax\n",
    "                logits = logits / temperature\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            if idx_next == stop_id:\n",
    "                break\n",
    "\n",
    "            # 将采样的索引添加到序列中并继续\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        # 只返回生成的token\n",
    "        return idx[:, index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd7c17",
   "metadata": {},
   "source": [
    "验证上面的代码是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a7907bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 82594560\n",
      "torch.Size([1, 1, 6144])\n"
     ]
    }
   ],
   "source": [
    "args = ModelConfig()\n",
    "\n",
    "# LLaMA2Model.forward 接受两个参数，tokens和targets，其中tokens是输入的张量, 应为int类型\n",
    "x = torch.randint(0, 6144, (1, 50)) # [bs, seq_len]\n",
    "# 实例化LLaMA2模型\n",
    "model = LLaMA2(args=args)\n",
    "# 计算model的全部参数\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters:', num_params)\n",
    "\n",
    "out = model(x)\n",
    "print(out.logits.shape) # [batch_size, 1, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92100c10",
   "metadata": {},
   "source": [
    "## 2 训练 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832b683",
   "metadata": {},
   "source": [
    "- Word-based Tokenizer：将文本按空格和标点符号分割成单词。无法处理未登录词、罕见词。\n",
    "- Character-based Tokenizer：将文本中的每个字符视为一个独立的 token。导致 token 序列变得非常长，增加了模型的计算复杂度和训练时间。\n",
    "- Subword Tokenizer：将文本分割成比单词更小的单位，但又比字符更大。\n",
    "    1. Byte Pair Encoding (BPE)：通过反复合并频率最高的字符或字符序列对生成子词词典。\n",
    "    2. WordPiece：通过最大化子词序列的似然函数来生成词典，但在合并子词时更注重语言模型的优化。\n",
    "    3. Unigram：通过选择具有最高概率的子词分割文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2df2ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.normalizers import NFKC\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb9ed524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 wikitext 数据集，包含了维基百科的文章文本。\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "013b19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练数据\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        yield dataset[i:i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10ef6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建配置文件\n",
    "def create_tokenizer_config(save_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    创建完整的tokenizer配置文件\n",
    "    :param save_dir: 保存路径\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False,\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"pad_token\": \"<|im_end|>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"model_max_length\": 1000000000000000019884624838656,\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"chat_template\": (\n",
    "            \"{% for message in messages %}\"\n",
    "            \"{% if message['role'] == 'system' %}\"\n",
    "            \"<|im_start|>system\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% elif message['role'] == 'user' %}\"\n",
    "            \"<|im_start|>user\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"<|im_start|>assistant\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% endif %}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            \"{{ '<|im_start|>assistant\\n' }}\"\n",
    "            \"{% endif %}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 保存主配置文件\n",
    "    with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 创建special_tokens_map.json\n",
    "    special_tokens_map = {\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<|im_end|>\",\n",
    "        \"additional_special_tokens\": [\"<s>\", \"</s>\"]\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(special_tokens_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1b96a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_from_jsonl(file_path: str) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    读取JSONL文件并安全提取文本数据\n",
    "    :param file_path: JSONL文件路径\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'text' not in data:\n",
    "                    raise KeyError(f\"Missing 'text' field in line {line_num}\")\n",
    "                yield data['text']\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON in line {line_num}\")\n",
    "                continue\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8d3002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(data_path: str, save_dir: str, vocab_size: int = 8192) -> None:\n",
    "    \"\"\"\n",
    "    训练并保存自定义tokenizer\n",
    "    :param data_path: 训练数据路径\n",
    "    :param save_dir: 保存路径\n",
    "    :param vocab_size: 词汇表大小\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 初始化tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()  # 添加文本规范化\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # 配置特殊token\n",
    "    special_tokens = [\n",
    "        \"<unk>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<|im_start|>\",\n",
    "        \"<|im_end|>\"\n",
    "    ]\n",
    "\n",
    "    # 配置训练器\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=2,  # 提高低频词过滤\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    "    )\n",
    "\n",
    "    # 训练tokenizer\n",
    "    print(f\"Training tokenizer with data from {data_path}\")\n",
    "    texts = read_texts_from_jsonl(data_path)\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))\n",
    "\n",
    "    # 验证特殊token映射\n",
    "    try:\n",
    "        assert tokenizer.token_to_id(\"<unk>\") == 0\n",
    "        assert tokenizer.token_to_id(\"<s>\") == 1\n",
    "        assert tokenizer.token_to_id(\"</s>\") == 2\n",
    "        assert tokenizer.token_to_id(\"<|im_start|>\") == 3\n",
    "        assert tokenizer.token_to_id(\"<|im_end|>\") == 4\n",
    "    except AssertionError as e:\n",
    "        print(\"Special tokens mapping error:\", e)\n",
    "        raise\n",
    "\n",
    "    # 保存tokenizer文件\n",
    "    tokenizer.save(os.path.join(save_dir, \"tokenizer.json\"))\n",
    "\n",
    "    # 创建配置文件\n",
    "    create_tokenizer_config(save_dir)\n",
    "    print(f\"Tokenizer saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f225a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tokenizer(tokenizer_path: str) -> None:\n",
    "    \"\"\"\n",
    "    评估tokenizer功能\n",
    "    :param tokenizer_path: tokenizer路径\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # 测试基本属性\n",
    "    print(\"\\n=== Tokenizer基本信息 ===\")\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "    print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "    print(f\"Special token IDs: {tokenizer.all_special_ids}\")\n",
    "\n",
    "    # 测试聊天模板\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个AI助手。\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm fine, thank you. and you?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm good too.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"That's great to hear!\"},\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== 聊天模板测试 ===\")\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        # add_generation_prompt=True\n",
    "    )\n",
    "    print(\"Generated prompt:\\n\", prompt, sep=\"\")\n",
    "\n",
    "    # 测试编码解码\n",
    "    print(\"\\n=== 编码解码测试 ===\")\n",
    "    encoded = tokenizer(prompt, truncation=True, max_length=256)\n",
    "    decoded = tokenizer.decode(encoded[\"input_ids\"], skip_special_tokens=False)\n",
    "    print(\"Decoded text matches original:\", decoded == prompt)\n",
    "\n",
    "    # 测试特殊token处理\n",
    "    print(\"\\n=== 特殊token处理 ===\")\n",
    "    test_text = \"<|im_start|>user\\nHello<|im_end|>\"\n",
    "    encoded = tokenizer(test_text).input_ids\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"Original: {test_text}\")\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    print(\"Special tokens preserved:\", decoded == test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39559e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tokenizer('your tokenizer path')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
