# Task04 图嵌入表示学习

## 1 回顾

- 图机器学习的任务：节点层面、连接层面、社交（子图）层面、全图层面
- 传统机器学习步骤：抽取D个特征编码成D维向量，再进行训练和预测

- 如何把节点映射成D维向量：
    - 人工特征工程：节点重要度、集群系数、Graphlet
    - 图表示学习：通过随机游走构造自监督学习任务，DeepWalk、Node2Vec
    - 矩阵分解
    - 深度学习：图神经网络

## 2 图嵌入简介

- 传统机器学习（特征工程）：抽取D个特征编码成D维向量，再使用机器学习算法进行训练和预测
- 图表示学习：
    - 不需要特征工程，将各个模态输入转为向量，自动学习特征
    - 将节点映射为d维向量，向量具有低维（向量维度远小于节点数）、连续（每个元素都是实数）、稠密（每个元素都不为0），与下游任务无关

- 嵌入d维空间：
    - 向量相似度反映节点相似度
    - 嵌入向量包含网络连接信息

## 3 基本框架（编码器+解码器）

- 编码器：输入一个节点，输出这个节点的D维向量，$\text{ENC}(v) = z_v$
- 解码器：输入这个节点的$d$维向量，输出节点相似度，向量点乘数值反映节点的相似度（需要人为定义）

$$
\text{similarity} (u, v) \approx z_v^T z_u
$$

- 优化目标：迭代优化每个节点的$d$维向量，使得图中相似节点向量数量积大，不相似节点向量数量积小

### 3.1 编码器

- 最简单的编码器：查表（浅编码器），采用独热编码，$\text{ENC}(v) = z_v = Z \cdot v$
- Z表示一个矩阵，每一列表示一个节点，行数表示向量的维度
- 优化Z矩阵的方法：DeepWalk、Node2Vec

### 3.2 解码器

- 基于节点相似度
- 目标：对$z_v^T z_u$进行优化迭代每个节点的D维向量，使得使得图中相似节点向量数量积大，不相似节点向量数量积小
- 直接优化嵌入向量，使用随机游走方式，如果两个节点出现在同一个随机游走序列中，就反映了这两个节点是相似的，并与下游任务无关

## 4 基于随机游走的方法

### 4.1 随机游走的概念

- 随机游走：可以定义具体的策略，在图中进行游走
- 图机器学习可以和NLP对应：
    - 图：文章
    - 随机游走序列：句子
    - 节点：单词
    - DeepWalk：Skip-Gram
    - Node Embedding：Word Embedding

### 4.2 随机游走的方法步骤

- $P(v|z_u)$：从$u$节点触发的随机游走序列经过$v$节点的概率
- 使用softmax方法计算$P(v|z_u)$：$\displaystyle \sigma(z)[i] = \frac{e^{z[i]}}{\displaystyle \sum_{j=1}^K e^{z[j]}}$

- 具体步骤：
    1. 采样得到若干随机游走序列，计算条件概率$P(v|z_u)$
    2. 迭代优化每个节点的D维，使得序列中共现节点向量数量积大，不共现节点向量数量积小

- 优点：表示能力、计算便捷、无监督/自监督学习问题

- 使用极大似然估计，优化目标函数
$$
\max \limits_f \sum_{u \in V} \log P(N_R(u) | z_u)
$$
其中$N_R(u)$表示从$u$节点出发的随机游走序列的所有邻域节点

- 整个优化的目标函数：
$$
\mathcal{L} = \sum_{u \in V} \sum_{v \in N_R(u)} -\log(P(v|z_u))
$$
其中
$$
P(v|z_u) = \frac{\exp (z_u^T z_v)}{\displaystyle \sum_{n \in V} \exp (z_u^T z_n)}
$$
遍历所有节点，并遍历从$u$节点出发的随机游走序列的所有邻域节点，计算节点$u$和节点$v$在该随机游走序列中共现。

### 4.3 计算优化

- 负采样：
$$
\log ( \frac{\exp (z_u^T z_v)}{\displaystyle \sum_{n \in V} \exp (z_u^T z_n)}) \approx \log(\sigma(z_u^T z_v)) - \sum_{i=1}^K \log(\sigma(z_u^T z_{n_i}))
$$
其中$n_i \sim P_V$（非均匀分布采样），$K=5 \sim 20$

- 梯度下降：
    - 全局梯度下降（Gradient Descent）：求所有节点$u$总梯度$\displaystyle \frac{\partial \mathcal{L}}{\partial z_u}$，并迭代更新$ \displaystyle z_u \leftarrow z_u - \eta \frac{\partial \mathcal{L}}{\partial z_u}$
    - 随机梯度下降（Stochastic Gradient Descent）：每次随机游走优化一次$\displaystyle  \frac{\partial \mathcal{L}^{(u)}}{\partial z_v}$，并迭代更新$\displaystyle z_v \leftarrow z_v - \eta \frac{\partial \mathcal{L}^{(v)}}{\partial z_v}$

## 5 Node2Vec

- 有偏二阶随机游走
- 通过两个超参数$p$和$q$控制随机游走的方向，其中概率$\displaystyle \frac{1}{p}$表示退回上一个节点，概率$\displaystyle \frac{1}{q}$表示走向更远的节点，1表示走向上一个节点距离相等的节点

- 设置不同的超参数：
    - $p$大$q$小：DFS深度优先（探索远方），应用于同质社群（homophily community）
    - $p$小$q$大：BFS广度优先（探索近邻），应用于节点功能角色（中枢、桥接、边缘）（structural equivalence）

- Node2Vec算法：
    1. 计算每条边的随机游走概率
    2. 以$u$节点为出发点，长度为$l$，生成$r$个随机游走序列
    3. 用随机梯度下降优化目标函数

## 6 矩阵分解角度（图嵌入和随机游走）

- 通过邻接矩阵分解$Z^T Z = A$，可得：
    - 两个节点之间相连：节点向量的数量积是1，两个节点是相似的
    - 两个节点之间不相连：节点向量的数量积是0，两个节点是不相似的

- 优化目标函数
$$
\min \limits_Z \|A - Z^T Z \|_2
$$

- DeepWalk的矩阵分解形式的目标函数：
$$
\log \left( vol(G) \left ( \frac{1}{T} \sum_{r=1}^T (D^{-1} A)^r \right ) D^{-1} \right) - \log b
$$
其中，$\displaystyle vol(G) = \sum_i \sum_j A_{ij}$表示连接个数的2倍，$T$表示上下文滑窗宽度，$T=|N_R(u)|$，$D$是对角矩阵，$r$表示邻接矩阵的幂，$b$表示负采样的样本数

## 7 随机游走的图嵌入的讨论与总结

- 基于随机游走的图嵌入的缺点：
    1. 随机游走的图嵌入方法都是对图中已有的节点计算特征，无法立刻泛化到新加入的节点，其实是某种程度的过拟合
    2. 只是探索相邻局部信息，只能采样出地理上相近的节点
    3. 仅利用图本身的连接信息，并没有使用属性信息

- DeepWalk：
   - 首个将深度学习和自然语言处理的思想用于图机器学习
   - 在稀疏标注节点分类场景下，嵌入性能卓越
   - 均匀随机游走，没有偏向的游走方向
   - 需要大量随机游走序列训练
   - 基于随机游走，管中窥豹，距离较远的两个节点无法相互影响。看不到全图信息。
   - 无监督，仅编码图的连接信息，没有利用节点的属性特征。
   - 没有真正用到神经网络和深度学习

- Node2Vec：
    - 解决图嵌入问题，将图中的每个节点映射为一个向量（嵌入）
    -  向量（嵌入）包含了节点的语义信息（相邻社群和功能角色）
    - 语义相似的节点，向量（嵌入）的距离也近
    - 向量（嵌入）用于后续的分类、聚类、Link Prediction、推荐等任务
    - 在DeepWalk完全随机游走的基础上，Node2Vec增加参数p和q，实现有偏随机游走。不同的p、q组合，对应了不同的探索范围和节点语义
    - DFS深度优先探索，相邻的节点，向量（嵌入）距离相近
    - BFS广度优先探索，相同功能角色的节点，向量（嵌入）距离相近
    - DeepWalk是Node2Vec在p=1，q=1的特例

## 8 嵌入整张图

- 所有节点的D维向量求和：$\displaystyle z_G = \sum_{v \in G} z_v$
- 引入虚拟节点，并求出虚拟节点嵌入，作为整个子图的嵌入

### 8.1 匿名随机游走

- 匿名随机游走嵌入：每次见到不同节点，就发一个新编号
- 3个节点的匿名随机游走有5种可能

### 8.2 Bag of Anonymous Walks

- 采样出不同匿名随机游走序列的个数，构建一个向量$z_G[i]$

- 当匿名随机游走长度固定时，使得误差满足大于$\varepsilon$的概率并小于$\delta$，则需要采样
$$
m = \left[ \frac{2}{\varepsilon^2} ( \log (2^{\eta} - 2) - \log(\delta)) \right]
$$

### 8.3 匿名随机游走的优化计算

- 给每种匿名随机游走序列单独嵌入编码，全图也单独嵌入编码
- 步骤：
    1. 给每种匿名随机游走序列单独嵌入编码，并表示为D维向量 $\displaystyle \frac{1}{\Delta} \sum_{i=1}^{\Delta} z_i$
    2. 全图进行编码，并表示为D维向量 $z_G$
    3. 将所有匿名随机游走序列编码表示的向量进行求平均，并与全图编码的向量进行拼接，得到2D维向量 $\displaystyle \text{cat} (\frac{1}{\Delta} \sum_{i=1}^{\Delta} z_i, z_G)$
    4. 进入线性分类层，预测出新的随机游走序列 $\displaystyle y(w_t) = b + U \cdot \left( \text{cat} ( \frac{1}{\Delta} \sum_{i=1}^{\Delta} z_i, z_G ) \right )$

- 优化目标函数：
$$
\max \limits_{z_i, z_G} \frac{1}{T} \sum_{t=\Delta}^T \log P(w_t | {w_{t - \Delta}, \cdots, w_{t-1}, z_G})
$$
其中
$$
P(w_t | {w_{t - \Delta}, \cdots, w_{t-1}, z_G}) = \frac{\exp(y(w_t))}{\displaystyle \sum_{i=1}^{\eta} \exp{y(w_i)}} \\
y(w_t) = b + U \cdot \left( \text{cat} ( \frac{1}{\Delta} \sum_{i=1}^{\Delta} z_i, z_G ) \right )
$$

## 9 本章总结

本次任务，主要讲解了随机游走和图表示学习，包括：

- 随机游走：生成短的固定长度的随机游走序列，以$u$节点作为出发点，得到该序列上的邻域节点，再使用随机梯度下降
- 图表示学习：无需人工特征工程，使用编码器和解码器，通过随机游走的方式构建自监督学习任务，可使用DeepWalk、Node2Vec算法
